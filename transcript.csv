Conv1|Hello world, it's arrived. And we're going to build a conversational network using no libraries, I mean, just numb pie, but no libraries, no TensorFlow, no pytorch, none of it, we're going to look at the math behind it. And we're going to build it with just non pie for matrix math in Python, okay, and what it's going to be able to do, let me just start off with this demo. To start off with, what it's going to be able to do is recognize any character that you type in, or not type in, but draw in with your mouse. So you could draw a six like that, and then hit submit, it'll start working. And then it'll say it's a six. And then if you don't want you to six, you could say a letter, like a, any number or letter, it's going to be able to detect, predict. So it's gonna be really cool, because we basically we're wrapping it into a web app using the flask web framework. So it's gonna be, it's gonna be super awesome. Okay, so that's what we're going to do today. And this is our first neural network that we're building in this course from scratch. I mean, we made one into weekly video, but this is the real, you know, hardcore, committed network with all the layers, all the functions, everything. Okay, so let's start off with what it's inspired by? Well, it's inspired by Yama Koon, the genius? No, it's not. So Yama, Kuhn is a director of AI at Facebook, he's a total G, he is awesome, because he was inspired by these original two guys, right here, who published a paper and I think 68, or early 60s or 70s, but the paper was on the mammalian visual cortex, and the idea they had was, and so here's a great image of it, let me make it a lot bigger, this has to be a lot bigger. So the idea they had was that mammals all see in a very similar way. And that way, is hierarchical. So you have a collection of cells, and the cells are neurons, and the cells cluster and, and these clusters represent different features that are learned, okay, so here in terms of neuroscience, they call these clusters, v1, v2, you know, have names for all these clusters in the brain, these clusters of neurons, before it post Syria, all this neuroscience terminology, but what we need to know is that at a high level, what's happening is, every time you see something, a a series of clusters or layers of neurons are being activated. Whenever you see something, whenever you detect something to be more accurate. If I detect a dog or a, you know, face or whatever, it's going to be a series of layers or clusters of neurons that fire and each of these clusters are going to detect a set of features. Okay, and these features are going to be more abstract the the higher up the hierarchy of clusters, you could think of it as a vertical hierarchy, or even a horizontal hierarchy, what it doesn't matter. But the idea is that there is a hierarchy of features. And at the start, these features are very simple, there are lines and edges, but then they get more abstract and they become shapes. And then they become
Conv2|more complex shapes. And eventually, at the at the highest level, at the highest cluster level exists the entire face or the entire dog, whatever it is. And this is how the mammalian visual cortex work. And so what young Koon said, and his team in 98, when they published probably the landmark paper of computational nets, which is kind of arguable, I guess, because it Khrushchev skis, image net paper was pretty good. And, and I think 2012, but anyway, Yakuza G, I just wanted to say that he had the idea to be inspired by three things, three features of the human, or the mammalian visual cortex, local connections, and that means the clusters between neurons, how each neuron, each set of neurons in a cluster cluster are connected to each other, and they represent some set of features. And then the idea of layering how these are, there's a hierarchy of features that are learned and spatial invariance. What does this mean this word spatial invariance, it means that, whenever you are I detect something, whether it's let's say, we're detecting a shoe, right, we, if you see a shoe, you know, it's a shoe, right? If it's easy, if it's, you know, Adidas, whatever it is, you know, it's a shoe, it can be shaved this way, or this way, it can be rotated or transformed, no matter how it varies, we still can detect that it's a shoe we know it's a shoe. So we are, it is the way its position, is it spatially invariant, we can still detect what it is. And so those three concepts, were what inspired the birth of convolution, neural networks, programmatic neural networks, designed to mimic the mammalian visual cortex. How cool is that? That's so cool. So how does this thing work? Let's look at how this works. So we have a set of layers, okay. And we'll talk about what these layers mean, right? What is layer, a layer, in each case is a series, it's a series of operations that we're applying. Okay, so let's, let's talk about this, right, so we have some input image. So let's see, this is the orange. That's the image. And you'll notice, by the way, that this image, this is a convolution network, by the way, this what we're building, okay, you'll notice that this image right here, or this image of the compositional network, isn't what you normally look at, when you think of neural network, right? You always see that image of the circles and everything's connected. So why is it different for convolution networks, because every layer and compositional network isn't connected to every up to every neuron in every layer isn't connected to every other neuron and the next layer, why? Because that would be too computationally expensive, I'll go over that in a second. But the idea is that if you if you see here, there's a part of the image that is connected this little square of that orange, and that is called the receptive field, okay, I'm going to go over all this, it's going to make more and more sense, you're going to be more confused, it's gonna be, it's gonna make more and more sense as I go further and further in depth here. So so so stay with me here. So we have a receptive field, okay, that is some part of the image that we are focused on, we are by focused, I mean, that is the part of the image that we apply a convolution operation. Okay, and we take that receptive field, and we slide it across the image, okay, you're going to see exactly what I'm talking about in a second, I'm just going to go over at a high level, we slide over the image, we are applying a dot product between our weight matrix data layer, and every part of that image iterative Lee okay. And so the reason that they look different the conventional networks look different is two reasons. Really, the first reason is that not every neuron and each layer is connected to every other neuron, the next layer, it's only a part of that, because it would be a to borrow from discrete math, a common editorial explosion, to connect every single pixel value in an image to every single pixel value in the next layer of features, right? It'll be just a huge amount. So what we do instead, is we take a part of that image and we intuitively slide over it. Okay, so at a high level, you understand the sliding part, right? Think of it as a flashlight. Okay, think of it. Think of the, the filter at each layer that shines over the recession, the field, that box as a flashlight, and you're shining over the image, and you're, you're applying products to all of these numbers, okay? Just like that. Okay, I'm going to keep going into this. That was just a high school. You're not supposed to understand it all yet. Okay. That was that was very high level, we're still going deeper. We're going deep. We're going deep. Okay, so check out this beautiful image right here. Isn't it beautiful? It's very beautiful. Also, you're beautiful for watching this. So thank you for watching this. Okay, so I love my fans so much. Seriously, you guys are amazing. Seriously, you guys are the reason I do this every week. Okay, so I By the way, I want to say one more thing to go on a tangent. The people who subscribe to my channel, no one thought they existed. We are programmers who are smart. And we are also cool. No one thought these people existed, but we exist. Okay, we are smart. And we are cool. So you are amazing. Okay, anyway, back to this. What this is is another way of looking at the network, right? We're just looking at different ways, we're looking at different ways. So we can build a specialty invariant image in our head of what a compositional network is like, right? No matter what the image is, we're going to learn to recognize a commercial network when we see one, I'm just trying to, you know, meta, applying this logic to what we're learning. So what happens is that each layer, we are applying a series of products between the way matrices and the input matrix, okay. And so what happens is, let's look at a third image. Okay, so that's a third image, what happens is we perform a series of operations, okay, at each layer. And so we could think of different, we could think of splitting up a convolution network into two separate categories. The first category is feature learning. And that's what's happening at the, at the, at the head of the head to the middle to almost the tail end of the network. And at the very tail end is classification. So there's two parts, there's the feature learning part. And then there's the classification part. And two, for the feature learning part, what happens are three operations over and over, over and over again, and we can call them convolution blocks, let's just call them convolution blocks. I'm coining the term. So what happens is, we first apply convolution, then we apply relay, or any kind of activation, and then we apply pooling. And we repeat that that's us, that's a single block three operations in a single computational block. Okay, so convolution, really pooling repeat convolution, really pooling, repeat competition really pooling, okay. And usually, you know, you have three blocks, at least, unless you're building Inception by Google, then you have 1515 of these. But you know, you have these computational blocks, and at the very end, then you flatten that output into a smaller dimensional vector, and then you apply a fully connected layer to it. So that means that you then connect all the neurons in one layer to the next one, just because we want to then harness all of the learnings that we've learned so far. That's why we fully connect at the end. And then we take those learnings, and we squash it into a setup probability values with our last soft max function. And then we take the max value of those probabilities. And each of these probabilities is a probability for a specific class that it could be and we think the max value, let's say 72%, as an will say, Okay, well, 72% for banana, and now we know it's a banana. Okay, so hopefully you get some of it. But it's very confusing. So I know we're about to go even deeper. Okay, so get ready for this. I haven't even started yet. So I haven't even started yet. Okay, so anyway, step one. So for step one, we are preparing a data set of images, right? So when you think of an image, you think of a matrix, hopefully a matrix of pixel values, if you don't think of it that way, think of think of it that way. Now, you're thinking of an image as a matrix of pixel values, rows by columns, and each of these, each of these
Conv3|points in the matrix represent a pixel right between zero and 255. But it's actually better in terms of computational networks to think of a an image as a three dimensional matrix. You're like what know what it's to know. So it's three dimensions. So the first dimension is the length of the image. The second dimension is the width. And the third dimension is the depth. So wait, what is the depth, because the depth represents the channels, and there are three channels for images, red, green, and blue, unless you're talking about grayscale, then there's black, then there's, you know, black and white, but we're talking about color images, okay. So there are three channels, and you have these dimensions for each of the channels. So these values in each of these, in each of these 2d matrices for and there are three of them represent the, the amount of redness or the amount of greenness or the amount of blueness between zero and 255. So in terms of commercial nets, we think of images as three dimensional pixels. Okay, so I wanted to say that part. Okay. So that's, that's, that's what we think of our image as our input image. And it has, it has an associated label, right? We're talking about super eyes learning, learning the mapping between the input data, and the output label, dog image, dog label, learn the mapping, given a new dog image, what is a label? Well, you just learned it, right? So and we learned it through back propagation back propagate to update weights. Remember the rhyme you know what it is? A I haven't wrapped yet in the series, but I will Don't worry, it's coming. Anyway. So every image is a matrix of pixel values. We know this, we know this between zero and 255. And we can use several training datasets, there are two really popular ones there see foreigner, and there's cocoa. And there's a bunch of other ones as well. But basically, these are huge datasets. And you can find smaller versions of them. And each of these images, their dogs, their cars, or airplanes, their people, whatever, they all have labeled for them. Handmade labels by humans, which is great for us. Okay, so that's, that's it. That's step one. Step one is to get your training data, which is your images, which are your images. Step two, is to perform convolution. Now, you might be asking, What is convolution? Well, I'm here to tell you that convolution is an operation that is dope as F. Here's why it's dope. Because it's not just used in computer science and machine learning. It's used in almost every field of engineering, think of convolution as two paint buckets, you have one paint bucket, which is red, and the other one, which is blue. And what you do is just smear it all over yourself. No, you don't do that, what you do is you take these two paint buckets, and you combine them into one paint bucket. And that new paint bucket is going to be a new color. Whatever that combination of colors is, that's convolution, convolution is taking two separate types of data, two matrices, and then apply, and then it's an operation that combines them. So you could think of convolution as synonymous to combination. Okay, and why do we apply? Why do we say that for convolution networks, because what we're doing is we are combining the values for each of these layers with the input matrix. So think of the input as that matrix, right? And so what it's a three dimensional, it's a, it's a 3d tensor, right, but we're applying it to each of these dimensions, right, so three of them. So just think of it as a matrix for right now. And so what we do is we take this, so at each layer, at each layer, there's a Wait, so by the way, okay, so there's a lot of interchangeable terms of machine learning. And it's easy to get confused here. But I want to set the record straight for a second wait is the same as feature matrix is the same as feature map is the same as a filter, in this case, infocomm social networks. So you see these or even Colonel Colonel is a different one, there's actually five interchangeable terms that can see how it can be confusing. But if you get the basic idea of you have an input matrix, which is your image, and then you have a set of matrices, which are your features that are learned, you know, edges, shapes, more abstract shapes. That's it, that's, that's all it is matrix dot product. matrices that are being multiplied by matrices all the way through, that's, that's all it is matrix, you've got to be multiplied by matrix these all the way through just a chain of them. Okay, so what happens for convolution is we take a matrix, and we multiply it by all the values in this matrix at a certain region, right. And so this is what I was talking about when I was saying we have a receptive field, because we don't just want to play all at once, we multiplied by a little part of it, okay, the receptive field, and we slide it and we can define what that interval is that sliding window, I know I'm talking about without coding, the coding is coming, believe me, the coding is coming. But just check this out for a second, we got to learn this, conceptually first.
Conv4|So we are multiplying the feature matrix by that input image just for every row and every column or just multiply multiply multiple. And what happens is, we have this new matrix that results the output, and that output is considered the convenience feature. Okay, it's what we do is we use that output as the input for them to the next layer. And we repeat the
Conv5|process over and over and over again,
Conv6|obviously, there's too many parts here. There's the activation, the Riu, and then there's the pooling, which I'll talk about as well. But that's the basic idea between convolution and that's what we call it convolution. Because we are combining or involving the wait matrix, or filter or kernel, whatever you want to call it feature map. By that input. We're combining using the app and using that output as the input for the next layer, after activating it and, and pulling it. Okay, so that's convolution, and also, right, so we apply to all of those dimensions for that for that input matrix. Okay, and that gives us our activation map or feature map or filter, right? So many different interchangeable terms here. So anyway, so it's computer using the doc product. So you might be thinking, Well, okay, I see how there's a dot product, I see how there's matrix multiplication. But how does that really tell us what features there are I still, you're still on making the connection, probably, why, understandably, why this, these series of operations help us detect features? Well, here's what happens. What happens is this. And here's the great thing about matrices, and having several of them. When we learn a filter, or wait, whatever you want to call it, you know what moving forward, let's just call it filter, okay, but I'm just saying, let's just call it filter moving forward. For the rest of this video. When we learn a filter over time, by training it on mouth mouth pictures, for example, a filter is going to look like this at let's say at the first layer, we we learn a filter for detecting a curve that looks like this, right, this curve right here. And so what's what this filter is going to look like for the technical specific type of curve, it's going to be a very sparse filter, that means there's a lot of zeros, except so there's all these zeros, except for right here, you see this 3030 3030. And notice that these values represent the shape, they go in this direction of a shape. And so what happens is, we take this filter and perform the dot product, you know, we can evolve it with whatever part of the mouse, if it's over a part of the mouse that matches that feature, exactly. Then we when we multiply all of those, when we when we perform the product between all those values, and sum them up, that's the convolution operation right there. Okay, just, it's going to be a big number. Okay, so then we know that we've detected a feature, because we've, we've multiplied it, sum it up, and there's a large number. And if there's not, if we multiply, if let's, let's say we had that receptive field over a different part of the mouse, and that that curve doesn't exist, then it's going to be zero, right? Because if you look between these 30 3030 values, and that the equivalent locations on this pixel representation of the mouse image, these are zeros. And so what happens when you multiply zero by 30, you get zero, right? So that's why it's important to make the rest of the so the data that's irrelevant, we want it to be zero, right? In the in the feature maps arts, the filters that we learn in the filters that we learn, we want the irrelevant parts to be zero, and in the images,
Conv7|okay. And in the input images.
Conv8|So I, so I can actually go even more into convolution, but it's not really necessary. But it's it is super dope. And it's super dope, though. This is a great blog post, by the way, I definitely encourage you to read this blog post, it's linked in the notebook. But this dude, Tim, Tim, he goes into these, this idea of convolution, and he talks about how its applied to all these different engineering fields. And he goes into the formula, the formula for the compositional theorem is what he called it is what it's called. Okay, and I'm just gonna go over this at a high level. But the convolution theorem is this general theorem for discrete while there's a discrete version and a continuous version, right? The screen is if there's, you know, one or zero black or white, you know, definite classes that something could be worse continuous Is it is it could be infinite amount of values between zero and 1.5, point two 5.7 infinity in that direction. But here's the here's the formula for it. And so let me make it bigger, just really quickly, and then we'll get back to it. Because it's, it's really cool. But the convolution theorem states that we and so in, it's a general theorem that can be applied to any any, any set of problems, but in terms of what's relevant to us, is, is the compositional theorem apply to matrix operations. So what we can do is we can say what it what it says is, it's the inputs times the kernel. And it's the product, it's a product between two different matrices. And we perform that for every value in all of those matrices. And we do that for all of the values that we have. And we sum them up together. And that's what the Sigma term represents. And we and we actually express that right here, right, this operation right here, this multiplication information is the same thing. But it's a more complex way of looking at it, or more matter, mathematically accurate way. And also, the fast Fourier transform is, is brought up by this. And that Fourier transform takes some spatial data, and it converts it into 48 space, which is like a waveform. And you see this a lot in your day to day life, whenever you're looking at some sound, you know, you're listening to some sound, and you look at your mp3 player and you see the waves. That's, that's a Fourier transform happening. But I won't go into that. That's, that's for sound and audio. But anyway, it's a really cool blog post, definitely check it out. Okay, so back to this. So we talked about convolution. Now we're going to talk about pooling, right? So what is pooling, so, whenever we apply convolution to some image, what's going to happen at every layer, is we're going to get a series of feature of, so each of the weights are going to consist of multiple images, and each of these images are going to be at every layer, there's going to be more and smaller images. So the first few layers are going to be these huge images, right? And then at the next few layers are going to be more of those, but they're gonna be smaller, and it's going to get just like that. Okay, and then we squash it with some fully connected layer. So it gets some probability values with the soft max. But anyway, what pooling does is it is it dense it is it makes the matrix, the major sees that we learn, more dense, here's what I mean. So if you, if you perform convolution between an input and a feature matrix, or a wait matrix, or filter, it's going to result in a matrix, right, but this matrix is going to be pretty big, it's gonna be a pretty big matrix, what we can do is we can take the most important parts of that matrix, and pass that on. And what that's going to do is it's going to reduce the computational complexity of our model. Okay, so that's what pulling is all about. It's a pulling set. So there's different types of pulling, Max pooling is the most use type of pulling by the way. So basically, multiply. So what happens is we we strive, we have some we define some window size, and then some stride size. So how, what are the intervals that we look at, and we say, okay, so for each of these windows, let's take the max value. So for, so for this one, right here, for 608, the max value will be eight, and 241 312, nine and be 12. Right? So we just take the biggest number, it's really simple, actually, we just take the biggest number. And we just do that for all of them. And so that that's what pulling is all about. And so it's going to just give us that the most relevant parts of the image. And if you if you think of these, these, these values in the, in the matrix as pixel intensities, by taking the maximum intense, the the pixel with the most intensity, or the highest intensity, we're getting that feature that is the most relevant, you see what i'm saying it's the least opaque feature to use a term from image. Math. Anyway, so we so we talked about pooling, and we talked about, we talked about activation. And so now now,
Conv9|we talked about convolution, and we talked about pooling.
Conv10|And so now the third part is normalization or activation. So remember how I said how it would be, it's so important that we have these values that are not related to our image be zero, we want it to be zero, so the result is zero. If the if the feature is not detected, well, the way we do that, is using review. And so really stands for rectified linear unit, it's an activation function. It's an activation function, okay, we use activation functions throughout New neural networks. And we use them because it is, you can also call them nonlinear, because they, they make our model able to learn non linear functions, not just linear functions, but non linear function. So any kind of function, right, the universal function approximation theorem, we talked about that activation functions help make this happen. And so really is this, this is a special kind of activation function that turns all negative numbers into zero. So that's why it's going to make the math easier, it won't make the math break for a conventional network. So apply really, so basically, what we do is for every single pixel value in the in the input to this really activation function, we turn it if it's a negative, we just say make zero, it's super simple, it will be one line of code, you'll see exactly what I'm talking about. Okay, so that's, that's those are our blocks. So that's how our convolution blocks work. However, there is another step that I didn't talk about, that is a nice to have, and state of the art commercial networks always use it. And that's called dropout. So Jeffrey Hinton, the guy who invented neural networks, invented a feature invented a technique called dropout. And what dropout is, is a good analogy is old people are not old people, but people who are stuck in their ways. Let me let me Okay, so what dropout does is it turns neurons on and off randomly. What do I mean by that? That I mean, the matrices for each weight value is converted to zero randomly at some layer of the network. And so what happens is, by doing this, our network is forced to learn new representations for the data, new pathways and that data has to flow through, it can't always flow through this neuron. And the reason we use it is to prevent overfitting, right, we want to prevent overfitting, we want to prevent being to fit to the data. Think of it as you know, the older you get, the more set in your ways of thinking you're you are, right. And so it's harder to think of new ways of thinking, right? Because you're so set in some ways. So a way to prevent that is to have a novel, crazy experience, whether it's skydiving or taking psychedelics, or whatever it is. And what that does is it creates new pathways. So you're not you're kind of forced, your brain is forced to make new pathways. And this increases your generalization ability, and you're not so overfit. That's a very rough, abstract analogy. But basically dropped out is not as complex as that sounds dropped out, can be done in three lines of code. So definitely check out this blog posts is wall that I've linked, but what it does is it just randomly pick some neurons in a layer to set to zero, right. So it's just, it's just three lines, okay, and you can look at it in this notebook, right. So that's, and then our last step is probability conversion. So we've got this huge set of values, right, all these little small images that are represented by this huge output matrix. And we want to take this huge set of values and make some sense out of it, we want to make probabilities out of it. And the way we do that is using a soft max at the end, a soft Max is a type of function and it looks like this, this, this is a soft max function right here. But what we do is we plug these values into the soft max function. And it's what you output a set of probability values, discrete probability values for each of the classes that we're trying to predict. Okay. And then what we'll do is given all those probability values will pick the biggest one using our max, the arc max function and n pi. And that's going to give us the most likely glass. Okay, those are the seven steps of official a full forward pass through accomplished network looks like that. And so now, you might be wondering, Well, okay, so how do we train this thing? Well, using gradient descent, right, and when applied to neural networks, graded gradient descent, is called. back propagation. Exactly. I hope you got that right. Anyway. Okay. So how do we learn these magic numbers, right? How do we learn what these wait value should be what the feature should be? back propagation is how we do it, right. And so we've talked quite a bit about back propagation and gradient descent, but I'll do a little, I'll go over it again.
Conv11|But the idea is that we have some error that we're computing, right? This is super, this is supervised learning, we have a, we have a human label, right for some data. So we put in a dog image, or a bicycle image to look at this image to look to relate to this image here, we put in a bicycle image in the bike label, we pass it through the each layer dot product dot product that you know, dot product activation function pool dot product, repeat p soft Max, or squash into probability values, pick the biggest one, and we have some prediction value. And what we do is we compare the prediction value to the the actual value and we get an error, and we take our error, and we compute the partial derivative of the error with respect to each wait value going backwards in the network, okay, like this. Okay, and so for regression, we use the means squared error if we're using linear regression regression. And for classification, we use the soft max function. So remember how in the first neural network we built, and in their linear regression example, we used a, we use means squared error to compute the air and now we're using the soft max. So we'll take the so we'll take the partial derivative of the air with respect to our weights, and then that's going to give us the gradient value that we then update each of those weight values, recursive Lee going backward in the network? And that's how it learns what those features, what the ideal feature, the weight matrix value should be. But what about the other? What about the other magic numbers? What about the number of neurons and the number of features and the size of those features and the pooling window size and the windows tried? Well, those that is an active area of research, there are best practices for values that you should use for those for those hyper parameters, right, the tuning knobs of our network, and Andre Karpati has some great material on this. He's probably the leading source for conversational networks right now, in terms of written content. And yeah, I mean, this is an active area of research, finding out what the ideal hyper parameters for our neural network should be. And we're still learning what it should be what what, what, how we can get them rather than just guessing and checking, which is what we do right now, which is kind of like, you know, not as not as optimal, right? So anyway, last two things, and then we're going to start with the code, when is a good time to use this what we know to classify images, we've talked about that, but you can also use them to generate images. And that's for later on, that's a little more advanced. But to give you a little spoiler a little teaser are, in fact, this is in my internet, deep learning playlist, taking compositional network, you flip it, and then you call it a D conversational network. And then you can take some text, and create an image out of text. How crazy is that? Okay. There's also generated models, where you have two networks fighting each other, and you can generate new images, whole bunch of really cool, crazy stuff you can do. But anyway, when should you use a conversational network? Anytime you have spatial, 2d or 3d data, what do I mean? Well, obviously, images are spatial. The word spatial implies that the space the positioning of the data matters. So sound, you can apply to sound images, or text or the the spit the position of the text matters, right, because we have a flashlight, or filter, and we're involving over an image, right. But if you have some data, like, say, customer data, or if you were to just flip the rows and columns, it doesn't matter what order they're in, they're still you know, they're still features. So a good rule of thumb is if you swap out the rose and columns of your data set, and it's just as useful, like the space doesn't matter, then you don't want to use a CNN helps you do. Okay, and a great and last thing, the great example of using cnn are for robot learning, you can use a CNN for object detection. And you can use a CNN for grasp learning and combine the two and you could get a robot that cooks, she's really cool. Got a great TensorFlow example, and a great adversarial network example. Okay, let's go into the code now. And so, what I'm going to do is I'm going to look at the class for the commercial networking on pi, as well as the prediction class. There's two classes here. Okay, so these are our three inputs. Pickle is for saving and loading our serialized model. What do I mean pickle is pythons way of having a platform or language agnostic way of saving data, so you can load it up later, TensorFlow uses it, a bunch of other libraries use it as well, known Pizer matrix math. And we've got our own little custom class for pre processing the data because we don't care about that part, we care about the machine learning part. Okay, so
Conv12|let's talk about our light OCR or object optical character recognition class. In our initialize function, we're going to load the weights from the pickle file, and then store and then store all the labels that we've loaded, will define how many rows and columns in an image, load up our commercial network using the light cnn function with our saved weights. So assuming we've already trained our network, we load it with the saved weights from the pickle file. And then we defined the number of pulling letters, okay, and so once we have that, then we can use this predict function. So given some new image will reshape the image shows in the correct size, perform the dot product between that image and the first layer of our commercial network. And will will, will put it Google feed it into our network. And it's going to output a prediction probability for a class and we're returning, okay, super high level, we haven't even coated our CNN. That's, that's our first class. That's our prediction class. Now, now we're going to look at the conversational network class. And what I'm going to do as I'm going to, I'm going to go over the code, and I'm going to code some parts of it. So now we'll look at our compositional network class, okay, so in our initialize function, will initialize to lyst. One to store the layers that we've learned the weights of each layer, and then the size of the pooling area for max pooling, okay, we'll load up our weights from our pickle file just like this. And then we have our predict function. Now in our predict function. That's where the real magic is happening, right? Let's code what this looks like. So given some input x, we're going to feed it through all of these layers, right? So what happens is, we will say, Okay, so the first layer is going to be a convolution layer. Okay, and we're going to define what all of these functions look like, look like, but the first layer is going to be that convolution layer will feed in that first image. And we'll say, Okay, well, this is the first layer. So it's a zero layer will say border mode equals full. And I'll talk about that part later on. But that's it for that. And so what happens is x equals this layer, okay, so that's our first layer. And then our next layer is going to be really, so we'll say, Okay, now let's apply an activation to the output of the previous layer, okay? And then we'll set it equal to that. Okay, so we'll set the output from the previous later equal to the input of this layer. And then we keep going, we say, Okay, so we've got another, CNN, we have another convolution layer. And we do the same thing here, we say, Okay, take the output from the previous layer will define what the name of this layer is, as well as the border mode, which I'll talk about the very end of this ever border mode, which is valid. And then we say, Okay, well hopes that the output of that equal to the input of this and just keep repeating, now it's time for us to apply a nother non linearity. So we'll just go ahead and apply our non linearity. Again, remember, these are delusional blocks. Oh, and we also want to pool so also the, the order with which you can do this varies, right, you can do this in different ways. And yeah, so I'm doing it a certain way, right now, you know, we could change it around, it would change our result. But the order map, the ordering within the block, it can be can be different. Okay, so, right, so we're going to pull it, we're going to pick the most relevant features from from that from that output, and then we're going to perform dropout to prevent overfitting. And we're going to say, there's going to be a point two 5% chance that a neuron is going to be deactivated, that will turn it off, set it to zero, and that's our dropout probability value. And then now we're getting into our, our, the second category of our network, not the feature learning part, but the classification part, it will say, Okay, so let's flatten this layer, let's reduce the dimensionality of all that data. So it's something that we can then learn from, don't say, well, let's, let's set it equal to seven. And then we'll say, once again, turn that output into our inputs here. Okay, so then we have another dense layer, we just, we just keep going with our first dense layer. And that means we're going to it's a fully connected layer. So we're combining everything that we've learned, because we're getting really close to squashing these values into a set of probability value. So we want to take all of our learnings and combine them with a fully connected layer. And so we'll combine them with a fully Connect layer. And then we'll squash it now with our sigmoid or not our sigmoid, our soft max function, okay. And then that's going to give us our output probability. And then we're going to say, well, which of the probabilities Do we want, we want the next one, right, we want the max probability. And we'll classify it just like that, and return that value. Okay, that's the highest level. And so if you were using Kairos, or one of these high level libraries, this is all your code would look like, we're going to do is we're going to look at these functions as well. Okay, so let's look at these functions.
Conv13|So start off with the compositional layer function and have your notebook open with me as well. So you could go over this, the link is in the description, if you don't know. Now, you know, if you don't know now, you know. So for our compositional layer, given some input image, we're going to say, well will store feature maps and the bias value in these two variables, features and bias will define how big our filter or patches going to be, how many features Do we want, how big is our image, how many channels RGB, so three, and then how many images Do we have. So given those values will define a border mode, so a border mode, so is, so when you apply full to border mode, in this case, it means that the filter has to go outside the bounds of the input by filter size divided by two, the area outside of the input is normally padded with zeros and the border mode valid is when you get an output that is smaller than the input, because the convolution is only computed where the input and the filter fully overlap. Okay, and they'll give us different, they'll give us different classification results, accuracy results, and it's good to test both options. So what we'll do is we'll initialize our feature matrix for this layer as, as involved zeros is going to be a bunch of zeros. And then we'll say okay, so for every image that we have, for every feature in that image, let's initialize a convulsed image as empty, and then for each channels are doing is for each of the three channels, let's extract a feature from our feature map, define a channel specific part of our image, and then perform convolution on our image using that given feature filter. So notice this involved 2d function, it's where the actual convolution operation is happening. This is more of a wrapper for that actual mathematical operation. So once we have that will add a bias and a bias x as our anchor for our network, it's kind of at the y intercept, it's kind of like a starting points for a model to exist. And then we'll add it to our list of features for this for this layer, okay, and we'll return that as the as our feature map are, our set of filter values are waiting matrices. And so let's look at this involved 2d function. So in our involve 2d function will define the tensor dimension of the image and the feature will get a target dimension. And then these two lines perform this, this operation is compositional theorem that we define right here, we're performing a product between the input and the kernel or feature for for all of those wait values, and then we're summing them all up. And that's going to be our output. And so the fast for a function and numb pie does this very well. And so we can just use that as FFT to. But that's what it's a multiplication and in summation operation, okay, and so then we have our target value. And then once we have our target value, we could say, Okay, let's have a starting point and an ending point. And our target values going to be within that range of what we want to return has the involved feature, right? So we have some bounding box that we want to apply this to. Okay, so then, so we have that. So what else do we have? So we start off with our conversational layer, and then we had our release. So what is really, really super simple really, really is just forgive. So for, for some matrix of zeros, will go through every single pixel value in the input matrix. And if it's a negative number, we just turn it into zero. That's it. That's really OK. And then, so we have we had talked about really, we've talked about convolution, we have to talk about pooling. So what is Max pooling look like. So given our learned features, and our images, let's initialize our more dense feature list is empty. So here's what we do, we're going to, we're going to take the max values of all of those parts of the input image, right, so we can say, we're going to say for each image, and for each feature map, begin by the row define a starting and ending point, okay, which we defined with our pool size, hyper parameter. And so for each color, we've got a set of rows and columns for each image, there's a notice a lot of nesting happening here, we're going to define starting point for the columns as well. And then we're going to say, define a patch given our defined starting and ending point, so some some bounding box, and then take the max value from that patch using NNP dot max. And that patch is what moves around, right? For all parts of that image, and then we returned that, and we're going to store all of that in our pool features matrix right here, and we return that as the output. And that's what we pass on in the delusional network. Okay, so that's what max pooling is. Okay, so we talked about convolution, really max pooling, and then drop out. So for dropouts,
Conv14|right, we have a probability value that we define as point two, five, and we just multiply it by the inputs, okay. And that would that's going to do is going to turn on or off some part of the matrix into so by hundred off, I mean, zero, don't make it either zero or not zero, so it'll, so then our data will have to learn to either be multiplied by it or find a different pathway. And that's for drop out. And then we talked about drop out and convolution, flattening dense and soft max. So for flattening, it's just a, it's a tensor transformation, we just reduce the dimensionality of the input. Okay. And then, for our dense layer, our dense is our fully connected layer. Now, this is the generic layer that you would see in a feed for network input times wait. And then you add a bias, right? The which is the product right here, this is this is a dense layer, which take our input times away at a bias. So that means we just perform the dark product between the full weight matrix and the full weight matrix, instead of doing it at all the layers because that would be way too computationally expensive. For image data, we perform it at one fully one fully connected or dense layer at the end. And that's a way for us to combine all of our learnings together, so we can then crumbly squash it with a soft max function. Okay, so then, for our soft max layer, and then we have classify the first soft max layer, we will. So this is the this is the formula for softmax programmatically speaking, but what it does is going to output a set of probability values. And then we'll classify those values by taking the max the largest probability, and that is our output. Okay, so that is our forward pass to the network. Okay.
Conv15|And so
Conv16|yes, that is our forward pass through the network.
Conv17|So back, so back propagation works pretty much the same way, as I've talked about before, several times, greatness and back propagation works the same way, we take the partial derivative of our hair with respect to our weights, and recursive Lee update our weights using that gradient value that we gradient equals partial derivative equals delta interchangeable words. But here's a great simple example right here, or we after the forward pass, we do the same thing in reverse order to calculate the gradient of those weights and then back and then multiply them by the previous layer. And then for our JavaScript portion, we are taking the drawing from the user, here's the main code for that paint window in a canvas. And we are going to say, capture the mouse is positions capture all those points in that image with an event listener. And they're going to say on paint. So whenever the user actually starts moving that painting, whenever that mouse stops clicking, and then the user hits the submit button will save that snapshot or that image and then feed that into the network. And that's our flask app will define two routes, one for our home and then one for that image for the network. We can deploy to the web, there's a Roku app, you can definitely check out the link link is in the description as well check out the notebook. And yeah, that's it. Please subscribe for more programming videos. And for now, gotta do a Fourier transform. So thanks for watching.
